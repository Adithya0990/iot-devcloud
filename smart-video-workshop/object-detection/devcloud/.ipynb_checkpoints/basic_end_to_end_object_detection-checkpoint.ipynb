{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Object detection with Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "This tutorial uses a Single Shot MultiBox Detector (SSD) on a trained mobilenet-ssd* model to walk you through the basic steps of using two key components of the Intel® Distribution of OpenVINO™ toolkit: Model Optimizer and Inference Engine.\n",
    "\n",
    "Model Optimizer is a cross-platform command-line tool that takes pre-trained deep learning models and optimizes them for performance/space using conservative topology transformations. It performs static model analysis and adjusts deep learning models for optimal execution on end-point target devices.\n",
    "\n",
    "Inference is the process of using a trained neural network to interpret data, such as images. This lab feeds a short video of cars, frame-by-frame, to the Inference Engine which subsequently utilizes an optimized trained neural network to detect cars.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Part 1: Optimize a deep-learning model using the Model Optimizer (MO)\n",
    "\n",
    "In this section, you will use the Model Optimizer to convert a trained model to two Intermediate Representation (IR) files (one .bin and one .xml). The Inference Engine requires this model conversion so that it can use the IR as input and achieve optimum performance on Intel hardware.\n",
    "\n",
    "\n",
    "### 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the model downloader tool we will download the required models that are helpful in this exercise. We will use the **MobileNet-SSD** model. Download the model, specifying the name and output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############|| Downloading topologies ||###############\n",
      "\n",
      "========= Downloading models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.prototxt\n",
      "... 100%, 28 KB, 64757 KB/s, 0 seconds passed\n",
      "\n",
      "========= Downloading models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel\n",
      "... 100%, 22605 KB, 22062 KB/s, 1 seconds passed\n",
      "\n",
      "\n",
      "###############|| Post processing ||###############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### 2. Run the Model Optimizer on the pretrained Caffe* model. This step generates one .xml file and one .bin file and place both files in the tutorial samples directory (located here: models//object-detection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u26212/15oct/iot-devcloud/smart-video-workshop/object-detection/devcloud/models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel\n",
      "\t- Path for generated IR: \t/home/u26212/15oct/iot-devcloud/smart-video-workshop/object-detection/devcloud/models/object_detection/common/mobilenet-ssd/FP32\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[127,127,127]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t256.0\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/home/u26212/15oct/iot-devcloud/smart-video-workshop/object-detection/devcloud/models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u26212/15oct/iot-devcloud/smart-video-workshop/object-detection/devcloud/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
      "[ SUCCESS ] BIN file: /home/u26212/15oct/iot-devcloud/smart-video-workshop/object-detection/devcloud/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
      "[ SUCCESS ] Total execution time: 4.70 seconds. \n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel -o models/object_detection/common/mobilenet-ssd/FP32 --scale 256 --mean_values [127,127,127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "   Note: Although this tutorial uses Single Shot MultiBox Detector (SSD) on a trained mobilenet-ssd* model, the Inference Engine is compatible with other neural network architectures, such as AlexNet*, GoogleNet*, MxNet* etc.\n",
    "\n",
    "\n",
    "The Model Optimizer converts a pretrained Caffe* model to make it compatible with the Intel Inference Engine and optimizes it for Intel® architecture. These are the files you would include with your C++ application to apply inference to visual data.\n",
    "\n",
    "   Note: if you continue to train or make changes to the Caffe* model, you would then need to re-run the Model Optimizer on the updated model.\n",
    "\n",
    "### 3. Navigate to the tutorial sample model directory and verify creation of the optimized model files (the IR files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet-ssd.bin  mobilenet-ssd.mapping  mobilenet-ssd.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/object_detection/common/mobilenet-ssd/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "You should see the following two files listed in this directory: mobilenet-ssd.xml and mobilenet-ssd.bin\n",
    "\n",
    "\n",
    "## Part 2: Use the mobilenet-ssd* model and Inference Engine in an object detection application\n",
    "\n",
    "\n",
    "### 1. Open the sample app [main.cpp](main.cpp)  and view the lines that call the Inference Engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Line 135 — loads the Inference Engine plugin for use within the application\n",
    "- Line 148 — initializes the network object\n",
    "- Line 215 — loads model to the plugin\n",
    "- Line 233 — allocate input blobs\n",
    "- Line 242 — allocate output blobs\n",
    "- Line 315 — runs inference using the optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this demo is separated into two parts. First part is responsible for reading the input stream and running the object detection inference workload on the stream. This part outputs Region Of Interest (ROI), in terms of coordinates, for each frame. The source code for this part can be found in [main.cpp](main.cpp) , and the executable will be named \"tutorial1\". Output ROI will be written into a text file, \"ROIs.txt\".\n",
    "\n",
    "The second part reads the ROIs.txt file, and overlays boxes on each frame of the stream based on the coordinates. Then the output video is written into a file. The source code for this step is in [ROI_writer.cpp](ROI_writer.cpp).\n",
    "\n",
    "We have provided a Makefile for compiling the examples. Run the following cell to compile the application. (tip: use crtl+enter to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -fPIE -O3 -fopenmp -o tutorial1 --std=c++11 main.cpp -I/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/include -I/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/include/cpp/ -I/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/samples/common/ -I/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/src/extension/ -I/opt/intel/openvino_2019.1.094/opencv/include -L/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/lib/intel64 -L/opt/intel/openvino_2019.1.094/opencv/lib -L/opt/intel/openvino/deployment_tools/inference_engine/samples/build/intel64/Release/lib/ -lgflags -linference_engine -ldl -lpthread -lcpu_extension_avx2 -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -lopencv_highgui -lopencv_videoio -lopencv_video -lgflags  \n",
      "g++ -fPIE -O0 -fopenmp -g -o ROI_writer --std=c++11 ROI_writer.cpp -I. \\\n",
      "            -I/opt/intel/openvino/opencv/include/ \\\n",
      "            -L/opt/intel/openvino/opencv/lib -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -lopencv_highgui -lopencv_videoio -lopencv_video -lgflags\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two executables, tutorial1 and ROIwriter, take a number of commandline arguments.\n",
    "\n",
    "Run the following cells to see the list of the available arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[usage]\r\n",
      "\ttutorial1 [option]\r\n",
      "\toptions:\r\n",
      "\r\n",
      "\t\t-h              Print a usage message\r\n",
      "\t\t-i <path>       Required. Path to input video file\r\n",
      "\t\t-model <path>   Required. Path to model file.\r\n",
      "\t\t-b #            Batch size.\r\n",
      "\t\t-thresh #       Threshold (0-1: .5=50%)\r\n",
      "\t\t-d <device>     Infer target device (CPU or GPU or MYRIAD)\r\n",
      "\t\t-fr #           maximum frames to process\r\n",
      "\t\t-o #            Path to the output ROIs file\r\n"
     ]
    }
   ],
   "source": [
    "!./tutorial1 -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[usage]\r\n",
      "\tROIviewer [option]\r\n",
      "\toptions:\r\n",
      "\r\n",
      "\t\t-h              Print a usage message\r\n",
      "\t\t-i <path>       Required. Path to input video file\r\n",
      "\t\t-ROIfile <path> Path to ROI file.\r\n",
      "\t\t-b #            Batch # to display.\r\n",
      "\t\t-l <path>       class labels file.\r\n",
      "\t\t-o <filename>       Output file path.\r\n",
      "\t\t-r <res>       (double) factor to cut reolution by; 2. will cut the resolution by half.\r\n",
      "\t\t-k <keepframe>       Writer will keep every <skipframe> frames.\r\n"
     ]
    }
   ],
   "source": [
    "!./ROI_writer -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Source your environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\r\n"
     ]
    }
   ],
   "source": [
    "! /opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the below cars video in this example to detect the cars from the input video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Sample Video</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay height=\"480\"><source src=\"cars_1900.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoHTML('Sample Video', \n",
    "          ['cars_1900.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### 4. Run the sample application to use the Inference Engine on the test video\n",
    "\n",
    "#### Create Job Script \n",
    "\n",
    "We will run the workload on several DevCloud's edge compute nodes. We will send work to the edge compute nodes by submitting jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "To pass the specific variables to the Python code, we will use following arguments:\n",
    "\n",
    "* `-f`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the optimized models XML\n",
    "* `-i`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the input video\n",
    "* `-r`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output directory\n",
    "* `-d`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hardware device type (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* `-n`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number of infer requests\n",
    "\n",
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing object_detection_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "ROIFILE=$OUTPUT_FILE/ROIs.txt\n",
    "OVIDEO=$OUTPUT_FILE/output.mp4\n",
    "\n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "./tutorial1 -i cars_1900.mp4 \\\n",
    "            -m models/object_detection/common/mobilenet-ssd/$FP_MODEL/mobilenet-ssd.xml \\\n",
    "            -d $DEVICE \\\n",
    "            -o $OUTPUT_FILE\\\n",
    "            -fr 3000 \n",
    "\n",
    "# Converting the text output to a video\n",
    "./ROI_writer -i cars_1900.mp4 \\\n",
    "             -o $OUTPUT_FILE \\\n",
    "             -ROIfile $ROIFILE \\\n",
    "             -l pascal_voc_classes.txt \\\n",
    "             -r 2.0 # output in half res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting a job to an edge compute node with an Intel Core CPU...\n",
      "60787.c003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba038fabecc1448b9efb1c8f05e6e78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ecf3f382c14fee8c6cc44101de4fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Rendering', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/core CPU FP32\" -N obj_det_core\n",
    "print(job_id_core[0])\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/core', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should see a video play with cars running on the highway and bounding boxes around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/core/stats.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6764899b0ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m videoHTML('IEI Tank (Intel Core CPU)',\n\u001b[1;32m      2\u001b[0m           \u001b[0;34m[\u001b[0m\u001b[0;34m'results/core/output.mp4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           'results/core/stats.txt')\n\u001b[0m",
      "\u001b[0;32m~/15oct/iot-devcloud/smart-video-workshop/demoTools/demoutils.py\u001b[0m in \u001b[0;36mvideoHTML\u001b[0;34m(title, videos_list, stats)\u001b[0m\n\u001b[1;32m     19\u001b[0m     '''\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/core/stats.txt'"
     ]
    }
   ],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)',\n",
    "          ['results/core/output.mp4'],\n",
    "          'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Part 3: Run the example on different hardware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "In the previous exercise we saw the time it took for inference in CPU, Now we will run the same sample on GPU and see the total time it takes to run the inference.\n",
    "\n",
    "#### GPU\n",
    "\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU.\n",
    "    \n",
    "    \n",
    "Set target hardware as GPU with -d GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/gpu GPU FP32\" -N obj_det_gpu\n",
    "print(job_id_gpu[0])\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/gpu', 'v_progress_'+job_id_gpu[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)',\n",
    "          ['results/gpu/output.mp4'],\n",
    "          'results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "The total time between CPU and GPU will vary depending on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/*/stats.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU')]\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/{arch}/stats.txt'.format(arch=arch), a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
