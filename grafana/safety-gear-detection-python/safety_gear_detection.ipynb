{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafana POC Demo: Safety Gear Detection\n",
    "\n",
    "\n",
    "This is a sample proof of concept demo for Intel Metering and Grafana integration using the safety gear detection demo.\n",
    "The demo is currently not available for all users.\n",
    "\n",
    "This demo assume that you have already gone through the Python safety gear detection example.\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the orignal video stream used for inference and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Workers video</h2>\n",
       "    \n",
       "    <video alt=\"\" controls autoplay height=\"480\"><source src=\"Safety_Full_Hat_and_Vest.mp4\" type=\"video/mp4\" /></video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ln -sf /data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\n",
    "videoHTML('Workers video', ['Safety_Full_Hat_and_Vest.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the models\n",
    "\n",
    "Just as in the main safety gear demo, we will use an existing Caffe model specially trained for detecting people, hardhats and safety vests.\n",
    "Let's convert the model using the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel\n",
      "\t- Path for generated IR: \t/home/u19887/Reference-samples/iot-devcloud/grafana/safety-gear-detection-python/models/mobilenet-ssd/FP32\n",
      "\t- IR output name: \tmobilenet-ssd\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t1.5.12.49d067a0\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u19887/Reference-samples/iot-devcloud/grafana/safety-gear-detection-python/models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
      "[ SUCCESS ] BIN file: /home/u19887/Reference-samples/iot-devcloud/grafana/safety-gear-detection-python/models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
      "[ SUCCESS ] Total execution time: 3.12 seconds. \n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "--model_name mobilenet-ssd \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : outout dirctory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `models/mobilenet-ssd/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
    "```\n",
    "These will be used later in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Meteraing for Inference on a video\n",
    "\n",
    "Statistic collection is done on the compute nodes using colectd service with Intel Metering module enabled. In this section, we will submit a job to the queue and log the usage statistics to a file remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating job file\n",
    "\n",
    "The collectd statistics can be accessed using the `curl` command.\n",
    "For example, here is the curl command for the last 100 seconds of cpu usage.\n",
    "\n",
    "```\n",
    "curl \"http://127.0.0.1/render?target=Collectdc003-n014.cpu*.*&from=-100seconds&format=json\" > ~/Metering/$PBS_JOBID.CPU.json\n",
    "```\n",
    "\n",
    "In the bellow job script, we query information from collectd at the end of the script. We measure the time it took for the python program (initialization included), then gather statistics by looking back for the same duration.\n",
    "Run this cell to create the job script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting object_detection_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/computer_vision_sdk/bitstreams/a10_vision_design_bitstreams/5-0_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "STIME=`date +%s`\n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/${FP_MODEL}/mobilenet-ssd.xml \\\n",
    "                                           -i $INPUT_FILE \\\n",
    "                                           -o $OUTPUT_FILE \\\n",
    "                                           -d $DEVICE \\\n",
    "                                           -l /data/reference-sample-data/extension/libcpu_extension.so \\\n",
    "                                           --labels /data/reference-sample-data/safety-gear-detection/labels.txt\n",
    "ETIME=`date +%s`\n",
    "let LOOKBACK_TIME=ETIME-STIME\n",
    "curl \"http://127.0.0.1:80/render?target=Collectdc003-n014.cpu*.*&from=-\"$LOOKBACK_TIME\"seconds&format=json\" > ~/Metering/$PBS_JOBID.CPU.json\n",
    "curl \"http://127.0.0.1:80/render?target=Collectdc003-n014.Metering_plugin.renderbusy&from=-\"$LOOKBACK_TIME\"seconds&format=json\" > ~/Metering/$PBS_JOBID.GPU.json\n",
    "curl \"http://127.0.0.1:80/render?target=Collectdc003-n014.memory.*&from=-\"$LOOKBACK_TIME\"seconds&format=json\" > ~/Metering/$PBS_JOBID.MEMORY.json\n",
    "curl \"http://127.0.0.1:80/render?target=Collectdc003-n014.thermal-thermal*.temperature&from=-\"$LOOKBACK_TIME\"seconds&format=json\" > ~/Metering/$PBS_JOBID.THERMAL.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only running the inference. But we are not rendering the video, as we are just interesrted in the inference part of the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "**Note** If you want to use your own video, Change the environment variable 'VIDEO' in the following cell from \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\" to the full path of your uploaded video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to submit the above job. Currently, only one node is supported.\n",
    "This node is a <a href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6062.c003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15970f767e9e4fb5884b513c91d6702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=c003-n014 -F \"results/ CPU FP32 $VIDEO\" -N obj_det_xeon\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU and using the onboard Intel GPU\n",
    "In the cell below, we submit a job to the same system. But in this case the inference workload will be run on the onboard Intel GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6063.c003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ac68490af946ddb61c2d9b6449f7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Inference', style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=c003-n014 -F \"results/ GPU FP32 $VIDEO\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bf67ee048d435eab7324f6588eb68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84d2dc6c6c94889950b5a5ccddf4e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs to complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the results are automatically uploaded to Grafana server at <a href=\"http://grafana.colfaxresearch.com\" >grafana.colfaxresearch.com</a>. The automation will create a dashboard for you with the name {User ID}.{Job ID} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
