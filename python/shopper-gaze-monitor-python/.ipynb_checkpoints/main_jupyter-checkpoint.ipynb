{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Aisle Monitor\n",
    "\n",
    "This reference implementation counts the number of people present in an image and generates a motion heatmap. It takes the input from a video file for processing. Snapshots of the output are taken at regular intervals and are uploaded to the cloud. It also stores the snapshots of the output locally.\n",
    "\n",
    "## Overview of how it works\n",
    "At start-up the sample application reads the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU and Intel® Movidius™ Neural Compute Stick 2.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results/[device] directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the orignal video stream used for inference and people counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf ./resources/store-aisle-detection.mp4\n",
    "videoHTML('Store Aisle Video', ['store-aisle-detection.mp4 '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Model\n",
    "\n",
    "This application uses the **person-detection-retail-0013** Intel® model, found in the **deployment_tools/intel_models** folder of the Intel® Distribution of OpenVINO™ toolkit installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Inference on a video\n",
    "\n",
    "Let's try running inference on video to see how the Intel® Distribution of OpenVINO™ toolkit works.\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to count the number of people present in an image and generates a motion heatmap.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "The inference code is already implemented in \n",
    "<a href=\"main.py\">main.py</a>.\n",
    "\n",
    "The Python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line argument options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "python3 main.py -m ${MODELPATH} \\\n",
    "                -i ${INPUT_FILE} \\\n",
    "                -o ${OUTPUT_FILE} \\\n",
    "                -d ${DEVICE} \\\n",
    "                -pt 0.7 \\\n",
    "                -l /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\n",
    "\n",
    "```\n",
    "To upload the results to the cloud, the Microsoft Azure storage name and storage key are provided as the command line arguments. Use `-an` and `-ak` options to specify Microsoft Azure storage name and storage key respectively.\n",
    "\n",
    "```\n",
    "python3 main.py -m ${MODELPATH} \\\n",
    "                -i ${INPUT_FILE} \\\n",
    "                -o ${OUTPUT_FILE} \\\n",
    "                -d ${DEVICE} \\\n",
    "                -pt 0.7 \\\n",
    "                -an <azure-account-name> \\\n",
    "                -ak <azure-account-key> \\\n",
    "                -l /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/lib/ubuntu_16.04/intel64/libcpu_extension_sse4.so\n",
    "\n",
    "```\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m pre-trained IR model found in the **deployment_tools/intel_models** folder of the Intel® Distribution of OpenVINO™ toolkit installation   \n",
    "\n",
    "* -i  location of the input video stream\n",
    "* -o location where the output file with inference needs to be stored (results/[device])\n",
    "* -d type of Hardware Acceleration (CPU, GPU, MYRIAD or HDDL)\n",
    "* -pt probability threshold value for the person detection\n",
    "* -l absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so )\n",
    "* -an microsoft azure storage name\n",
    "* -ak microsoft azure storage key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating job file\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable processor, where the Notebook is allocated a single core. \n",
    "To run inference on the entire video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"store_aisle_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
