{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO tutorial for OCR\n",
    "\n",
    "We will run an OCR example on OpenVINOâ„¢.  We will use the Convolutional Recurrent Neural Networks (CRNN) for Scene Text Recognition from the following github page : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n",
    "\n",
    "To obtain the frozen model necessary to start with OpenVINO from the github repositery, please look at our [documentation](https://docs.openvinotoolkit.org/R5/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_CRNN_From_Tensorflow.html) \n",
    "\n",
    "In this tutorial, we will show you first how to convert the TF frozen model through the Model Optimizer, then we will perform inference on the CPU (first Xeon and then Core).\n",
    "As the CRNN includes a LSTM cell, the inference can only be performed on CPU (only hardware plugin to support this layer yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup the Python environement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Optimizer\n",
    "\n",
    "\n",
    "Model Optimizer creates Intermediate Representation (IR) models that are optimized for inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u19892/Downloads/FromSeverine/workshop/workshop/model/crnn.pb\n",
      "\t- Path for generated IR: \t/home/u19892/Downloads/FromSeverine/workshop/workshop/model/FP32\n",
      "\t- IR output name: \tcrnn\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Offload unsupported operations: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t1.5.12.49d067a0\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u19892/Downloads/FromSeverine/workshop/workshop/model/FP32/crnn.xml\n",
      "[ SUCCESS ] BIN file: /home/u19892/Downloads/FromSeverine/workshop/workshop/model/FP32/crnn.bin\n",
      "[ SUCCESS ] Total execution time: 13.61 seconds. \n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "--input_model model/crnn.pb \\\n",
    "--data_type FP32 \\\n",
    "-o model/FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : output directory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `model/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/FP32/crnn.xml\n",
    "models/FP32/crnn.bin\n",
    "```\n",
    "These will be used later in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference Engine\n",
    "\n",
    "Now, we will run the inference on this model by building progressively the Python sample required to perform inference. \n",
    "This part of exercise feaures our Python API, similar functionalities can be found in our C++ API too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do OCR on the following input image, which obviously reads as **Industries**.\n",
    "![Image](board4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import cv2\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from time import time\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "from local_utils import log_utils, data_utils\n",
    "from local_utils.config_utils import load_config\n",
    "import os.path as ops\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define variables  like model path, target device, the codec for letter conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml='model/FP32/crnn.xml'\n",
    "device_arg='CPU'\n",
    "input_arg=['board4.jpg']\n",
    "iterations=1\n",
    "perf_counts=False\n",
    "\n",
    "codec = data_utils.TextFeatureIO(char_dict_path='Config/char_dict.json',ord_map_dict_path=r'Config/ord_map.json')\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plugin initialization for specified device and load extensions library if specified\n",
    "Now we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "The following cell constructs **`IEPlugin`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = IEPlugin(device=device_arg, plugin_dirs='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read IR\n",
    "We can import optimized models (weights) from step 1 into our neural network using **`IENetwork`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "net = IENetwork(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing input blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "net.batch_size = len(input_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and pre-process input images\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Image board4.jpg is resized from (90, 489) to (32, 100)\n",
      "[ INFO ] Batch size is 1\n"
     ]
    }
   ],
   "source": [
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "images = np.ndarray(shape=(n, c, h, w))\n",
    "for i in range(n):\n",
    "\timage = cv2.imread(input_arg[i])\n",
    "\tif image.shape[:-1] != (h, w):\n",
    "\t\tlog.warning(\"Image {} is resized from {} to {}\".format(input_arg[i], image.shape[:-1], (h, w)))\n",
    "\t\timage = cv2.resize(image, (w, h))\n",
    "\timage = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "\timages[i] = image\n",
    "log.info(\"Batch size is {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model to the plugin\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = plugin.load(network=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Inference\n",
    "We can now run the inference on the object  **`exec_net`** using the function infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Average running time of one iteration: 67.7804946899414 ms\n"
     ]
    }
   ],
   "source": [
    "infer_time = []\n",
    "for i in range(iterations):\n",
    "\tt0 = time()\n",
    "\tres = exec_net.infer(inputs={input_blob: images})\n",
    "\tinfer_time.append((time()-t0)*1000)\n",
    "\n",
    "res = res[out_blob]\n",
    "    \n",
    "log.info(\"Average running time of one iteration: {} ms\".format(np.average(np.asarray(infer_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing output blob\n",
    "The network outputs a tensor of dimension 25 (string length) * 37 (dimension of character space).\n",
    "First, we will go through the 25 characters and extracts the highest probability in the character space and its index in this space. \n",
    "We use the encoding files from the Github page to recover the mapping from index to character. (0&rarr;\"a\",36&rarr;\" \")\n",
    "In the github page, they also remove the consecutive duplicates and the space char, therefore we also perform this postprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is : industries\n"
     ]
    }
   ],
   "source": [
    "preds = res.argmax(2) ## extract highest probability in the second dimension\n",
    "preds = preds.transpose(1, 0)\n",
    "preds = np.ascontiguousarray(preds, dtype=np.int8).view(dtype=np.int8) # reformat to an array \n",
    "values=codec.writer.ordtochar( preds[0].tolist()) # map from index to character\n",
    "values=[v for i, v in enumerate(values) if i == 0 or v != values[i-1]] # remove duplicates\n",
    "values = [x for x in values if x != ' '] # remove space char (was character from index 36)\n",
    "res=''.join(values)\n",
    "print(\"The result is : \" + res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Job submission\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated a single core. \n",
    "We will run the workload on other edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"ocr_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ocr_job.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ocr_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "   \n",
    "# Running the object detection code\n",
    "# -l /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples/build/intel64/Release/lib/libcpu_extension.so \\\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 classification_sample.py  -m model/$3/crnn.xml  \\\n",
    "                                           -i board4.jpg \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit ocr_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [ocr.sh](ocr_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU,GPU,MYRIAD)\n",
    "3. the floating precision to use for inference\n",
    "4. the path to the input video\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11      properties = compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,8gb,1gbe,hddl-f,iei-mustang-f100-a10,iei-tank-fpga\r\n",
      "     15      properties = compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,8gb,1gbe,hddl-r,iei-mustang-v100-mx8\r\n",
      "     49      properties = compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,8gb,1gbe,iei-tank-core\r\n",
      "     12      properties = compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,8gb,1gbe,ncs,intel-ncs2\r\n",
      "      4      properties = compnode,iei,tank-870,intel-core,i5-6500te,skylake,intel-hd-530,8gb,1gbe,ncs,intel-ncs,iei-tank-movidius\r\n",
      "     10      properties = compnode,iei,tank-870,intel-core,i5-7500t,kaby-lake,intel-hd-630,8gb,1gbe\r\n",
      "     14      properties = compnode,iei,tank-870,intel-xeon,e3-1268l-v5,skylake,intel-hd-p530,32gb,1gbe,iei-tank-xeon\r\n",
      "      1      properties = compnode,jwip,intel-atom,e3950,apollo-lake,intel-hd-505,4gb,1gbe\r\n",
      "      1      properties = compnode,jwip,intel-core,i5-7500,kaby-lake,intel-hd-630,8gb,1gbe\r\n",
      "     15      properties = compnode,up-squared,grove,intel-atom,e3950,apollo-lake,intel-hd-505,4gb,1gbe\r\n"
     ]
    }
   ],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 3.2 Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Intel Skylake Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting a job to an edge compute node with an Intel Core CPU...\n",
      "8739.c003\n"
     ]
    }
   ],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub ocr_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/skylake-core CPU FP32\" $VIDEO -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if not job_id_core:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Intel Xeon E3 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting a job to an edge compute node with an Intel Xeon CPU...\n",
      "8740.c003\n"
     ]
    }
   ],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub ocr_job.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"results/xeon CPU FP32\" $VIDEO -N obj_det_xeon\n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if not job_id_xeon:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Intel KabyLake Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting a job to an edge compute node with an Intel Core CPU...\n",
      "8741.c003\n"
     ]
    }
   ],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub ocr_job.sh -l nodes=1:tank-870:i5-7500t -F \"results/kabylake-core CPU FP32\" $VIDEO -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if not job_id_core:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46c6dbff3cb4595814d0ded25d6b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c6077abd804c038bd0dc8b85b877f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output result, which can be found inside the results/core/result.txt file.\n",
    "Run the cells below to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industries\n",
      " Inference performed in 81.58612251281738ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/skylake-core/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industries\n",
      " Inference performed in 90.15655517578125ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/xeon/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industries\n",
      " Inference performed in 184.9050521850586ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/kabylake-core/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the output result you should have got is: **industries**, matching the input image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
