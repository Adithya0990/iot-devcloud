{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Segmentation (BraTS) with OpenVINO™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code example, we apply the U-Net architecture to segment brain tumors from raw MRI scans. With relatively little data we are able to train a U-Net model to accurately predict where tumors exist. \n",
    "\n",
    "The Dice coefficient (the standard metric for the BraTS dataset used in the study) for our model is about 0.82-0.88.  Menze et al. [reported](http://ieeexplore.ieee.org/document/6975210/) that expert neuroradiologists manually segmented these tumors with a cross-rater Dice score of 0.75-0.85, meaning that the model’s predictions are on par with what expert physicians have made. The below MRI brain scans highlight brain tumor matter segmented using deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration objectives\n",
    "* Healthcare use-case demo\n",
    "* **Model Optimizer** in action\n",
    "* U-Net based segmentation on edge hardware\n",
    "* Inferencing with OpenVINO™ Inference Engine\n",
    "* Running inference across CPU, integrated GPU, VPU, and FPGA and comparing throughput and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is U-Net?\n",
    "Since its introduction two years ago, the [U-Net](https://arxiv.org/pdf/1505.04597.pdf0) architecture has been used to create deep learning models for segmenting [nerves](https://github.com/jocicmarko/ultrasound-nerve-segmentation) in ultrasound images, [lungs](https://www.kaggle.com/c/data-science-bowl-2017#tutorial) in CT scans, and even [interference](https://github.com/jakeret/tf_unet) in radio telescopes.\n",
    "\n",
    "U-Net is designed like an [auto-encoder](https://en.wikipedia.org/wiki/Autoencoder). It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape.  However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. For each pixel in the original image, it asks the question: “To which class does this pixel belong?” This flexibility allows U-Net to predict different parts of the tumor simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/unet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cells to import dependencies (select the cell and use **Ctrl+enter** to run the cell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time you run this notebook this step will take a few minutes, as we will be installing `keras` and `psutil` locally to your instance in the DevCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import keras\n",
    "except:\n",
    "    print(\"Keras not installed\")\n",
    "    !{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import psutil\n",
    "except:\n",
    "    print(\"psutil not installed\")\n",
    "    !{sys.executable} -m pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "\n",
    "The Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes a utility script `model_downloader.py` that you can use to download some common modes. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet-121\r\n",
      "densenet-161\r\n",
      "densenet-169\r\n",
      "densenet-201\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "mtcnn-p\r\n",
      "mtcnn-r\r\n",
      "mtcnn-o\r\n",
      "mobilenet-ssd\r\n",
      "vgg19\r\n",
      "vgg16\r\n",
      "ssd512\r\n",
      "ssd300\r\n",
      "inception-resnet-v2\r\n",
      "dilation\r\n",
      "googlenet-v1\r\n",
      "googlenet-v2\r\n",
      "googlenet-v4\r\n",
      "alexnet\r\n",
      "ssd_mobilenet_v2_coco\r\n",
      "resnet-50\r\n",
      "resnet-101\r\n",
      "resnet-152\r\n",
      "googlenet-v3\r\n",
      "se-inception\r\n",
      "se-resnet-101\r\n",
      "se-resnet-152\r\n",
      "se-resnet-50\r\n",
      "se-resnext-50\r\n",
      "se-resnext-101\r\n",
      "Sphereface\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v2\r\n",
      "faster_rcnn_inception_v2_coco\r\n",
      "deeplabv3\r\n",
      "ctpn\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "faster_rcnn_resnet101_coco\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "age-gender-recognition-retail-0013-fp16\r\n",
      "emotions-recognition-retail-0003\r\n",
      "emotions-recognition-retail-0003-fp16\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-adas-0001-fp16\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0004-fp16\r\n",
      "face-person-detection-retail-0002\r\n",
      "face-person-detection-retail-0002-fp16\r\n",
      "face-reidentification-retail-0095\r\n",
      "face-reidentification-retail-0095-fp16\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "facial-landmarks-35-adas-0002-fp16\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "head-pose-estimation-adas-0001-fp16\r\n",
      "human-pose-estimation-0001\r\n",
      "human-pose-estimation-0001-fp16\r\n",
      "landmarks-regression-retail-0009\r\n",
      "landmarks-regression-retail-0009-fp16\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "license-plate-recognition-barrier-0001-fp16\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001-fp16\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "pedestrian-detection-adas-0002-fp16\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-attributes-recognition-crossroad-0230-fp16\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0005-fp16\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0002-fp16\r\n",
      "person-detection-retail-0013\r\n",
      "person-detection-retail-0013-fp16\r\n",
      "person-reidentification-retail-0031\r\n",
      "person-reidentification-retail-0031-fp16\r\n",
      "person-reidentification-retail-0076\r\n",
      "person-reidentification-retail-0076-fp16\r\n",
      "person-reidentification-retail-0079\r\n",
      "person-reidentification-retail-0079-fp16\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-0078-fp16\r\n",
      "road-segmentation-adas-0001\r\n",
      "road-segmentation-adas-0001-fp16\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "semantic-segmentation-adas-0001-fp16\r\n",
      "single-image-super-resolution-1033\r\n",
      "single-image-super-resolution-1033-fp16\r\n",
      "text-detection-0002\r\n",
      "text-detection-0002-fp16\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-attributes-recognition-barrier-0039-fp16\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-detection-adas-0002-fp16\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "vehicle-license-plate-detection-barrier-0106-fp16\r\n",
      "face-detection-adas-binary-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1032-fp16\r\n",
      "action-recognition-0001-encoder\r\n",
      "action-recognition-0001-encoder-fp16\r\n",
      "instance-segmentation-security-0049\r\n",
      "instance-segmentation-security-0049-fp16\r\n",
      "vehicle-detection-adas-binary-0001\r\n",
      "driver-action-recognition-adas-0002-decoder\r\n",
      "driver-action-recognition-adas-0002-decoder-fp16\r\n",
      "pedestrian-detection-adas-binary-0001\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-action-recognition-teacher-0002-fp16\r\n",
      "instance-segmentation-security-0033\r\n",
      "instance-segmentation-security-0033-fp16\r\n",
      "action-recognition-0001-decoder\r\n",
      "action-recognition-0001-decoder-fp16\r\n",
      "text-recognition-0012\r\n",
      "text-recognition-0012-fp16\r\n",
      "driver-action-recognition-adas-0002-encoder\r\n",
      "driver-action-recognition-adas-0002-encoder-fp16\r\n",
      "gaze-estimation-adas-0002\r\n",
      "gaze-estimation-adas-0002-fp16\r\n",
      "resnet50-binary-0001\r\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download model using the model downloader you will need to run the following command:\n",
    "`!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name <model name> -o <output directory>`\n",
    "\n",
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Generate FP32 Optimized Model\n",
    "Now, let's convert the model to the optimized model using the model optimizer. This model works best on most hardware, including CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/data/Healthcare_app/data/saved_model_frozen.pb\n",
      "\t- Path for generated IR: \t/home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP32\n",
      "\t- IR output name: \tsaved_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,144,144,4]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP32/saved_model.xml\n",
      "[ SUCCESS ] BIN file: /home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP32/saved_model.bin\n",
      "[ SUCCESS ] Total execution time: 5.09 seconds. \n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "            --input_model /data/Healthcare_app/data/saved_model_frozen.pb \\\n",
    "            --input_shape=[1,144,144,4] \\\n",
    "            --data_type FP32  \\\n",
    "            --output_dir output/IR_models/FP32  \\\n",
    "            --model_name saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : output directory (If this directory does not exist, it will be created for you.)\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "Running that command will produce two files:\n",
    "```\n",
    "output/IR_models/FP32/saved_model.xml\n",
    "output/IR_models/FP32/saved_model.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "### 1.2: Generate FP16 Optimized Model\n",
    "\n",
    "We will also be needing the FP16 version of the model for the calculations on the VPU architecture. Run the following command to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/data/Healthcare_app/data/saved_model_frozen.pb\n",
      "\t- Path for generated IR: \t/home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP16\n",
      "\t- IR output name: \tsaved_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,144,144,4]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP16/saved_model.xml\n",
      "[ SUCCESS ] BIN file: /home/u27467/iot-devcloud/python/Healthcare/output/IR_models/FP16/saved_model.bin\n",
      "[ SUCCESS ] Total execution time: 5.09 seconds. \n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "            --input_model /data/Healthcare_app/data/saved_model_frozen.pb  \\\n",
    "            --input_shape=[1,144,144,4] \\\n",
    "            --data_type FP16  \\\n",
    "            --output_dir output/IR_models/FP16  \\\n",
    "            --model_name saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting healthcare_job_openvino.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile healthcare_job_openvino.sh\n",
    "\n",
    "# Prevent error and output files from being saved to DevCloud\n",
    "#PBS -e /dev/null\n",
    "#PBS -o /dev/null\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "DEVICE=$1\n",
    "RESULTS=$2\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_bitstreams/2019R1_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "if [ \"$DEVICE\" = \"MYRIAD\" ] || [ \"$DEVICE\" = \"HDDL\" ]; then\n",
    "    FP_MODEL=\"FP16\"\n",
    "else\n",
    "    FP_MODEL=\"FP32\"\n",
    "fi\n",
    "\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 healthcare_openvino.py     -d $DEVICE \\\n",
    "                                   -IR output/IR_models/${FP_MODEL}/saved_model \\\n",
    "                                   -l /opt/intel/openvino/inference_engine/lib/intel64/libcpu_extension_avx2.so \\\n",
    "                                   -r $RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Inference Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create job files and submit them to different edge compute nodes. They will go into a queue and run once the compute resources are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27286.c003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b64c0866274802845fdcfca0cdf743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, bar_style='info', description='Processing', style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_id_core = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:i5-6500te  -F \"CPU results/\"\n",
    "print(job_id_core[0]) \n",
    "if job_id_core:\n",
    "    progressIndicator('results/'+job_id_core[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_xeon = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:e3-1268l-v5  -F \"CPU results/\"\n",
    "print(job_id_xeon[0]) \n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/'+job_id_xeon[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"GPU results/\"\n",
    "print(job_id_gpu) \n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/'+job_id_gpu[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"MYRIAD results/\"\n",
    "print(job_id_ncs2[0]) \n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/'+job_id_ncs2[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X  Vision Processing Unit (VPU))\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_hddlr = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-v100-mx8 -F \"HDDL results/\"\n",
    "print(job_id_hddlr[0]) \n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/'+job_id_hddlr[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_fpga = !qsub healthcare_job_openvino.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"HETERO:FPGA,CPU results/\"\n",
    "print(job_id_fpga[0]) \n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/'+job_id_fpga[0], 'i_progress.txt', \"Processing\", 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Image Comparison\n",
    "Here, we visualize our predictions. We can observe the frame rate, execution time, and dice coefficient (a value that describes the similarity between the ground truth and the prediction, with 1.0 indicating 100% accuracy). Note that it may take a few seconds to display the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Tank (Intel Core CPU)',\n",
    "          'results/'+job_id_core[0], '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Tank Xeon (Intel Xeon CPU)',\n",
    "          'results/'+job_id_xeon[0], '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Intel GPU (Intel Core + Onboard GPU)',\n",
    "          'results/'+job_id_gpu[0], '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Tank + Intel CPU + Intel NCS2',\n",
    "          'results/'+job_id_ncs2[0], '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Tank + IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))',\n",
    "          'results/'+job_id_hddlr[0], '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputHTML('IEI Tank + IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)',\n",
    "          'results/'+job_id_fpga[0], '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Architecture Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we benchmark the processing time and frames per second on different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('ncs2', 'Intel\\nNCS2'),\n",
    "             ('hddlr', ' IEI Mustang\\nV100-MX8\\nVPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/'+vars()['job_id_'+arch][0]+'/stats.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
