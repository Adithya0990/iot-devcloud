{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo: Car Detection\n",
    "\n",
    "This is a sample reference implementation to showcase Object detection (car in this case) with SSD and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete but continue doing things ont he host while accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel infer requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of capturing.\n",
    "\n",
    "## Overview of How it works?\n",
    "At start-up the sample application reads the equivalent of command line arguments  and loads a network and image from the video input to the Inference Engine (IE) plugin. \n",
    "A job is submitted to the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Core CPU, Intel® Movidius™ and/or Neural Compute Stick)\n",
    "After the inference is completed, the output videos are appropriately stored in the /results directory which can then be viewed within the Jupyter Notebook instance\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed actual Edge hardware\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import some dependencies needed for displaying the results in this notebook\n",
    "(tip: use **crtl+enter** to run the cell but stay on the same cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demoutils import videoHTML,liveQstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If the user is curious to see the input video, run the following cell to view the orignal video used for inference and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('cars video', 'cars_1900.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Using OpenVINO\n",
    "\n",
    "The complete listing of source code for this example is in [object_detection_demo_ssd_async.py](object_detection_demo_ssd_async.py).\n",
    "\n",
    "Let's follow the main aspects of this code to see how OpenVINO works.\n",
    "The application reads equivalent of command line arguments and loads a network to the Inference Engine, After getting a frame from the OpenCV's VideoCapture API, it is ready to perform Inference.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "python3 object_detection_demo_ssd_async.py -m mobilenet-ssd/$3/mobilenet-ssd.xml \n",
    "                                           -i video/cars_1900.mp4\n",
    "                                           -o $1\n",
    "                                           -d $2\n",
    "                                           -l extension/libcpu_extension.so\n",
    "\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **mobilenet-ssd** pre-trained model which has been pre-processed using the **model optimzer**\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "   (**Note** we are using mobilenet-ssd in this example. However, OpenVINO's Inference Engine is compatible with other neural network architectures such as AlexNet*, GoogleNet*, MxNet* etc.,)    \n",
    "\n",
    "* -i location of the input video stream (video/cars_1900.mp4)\n",
    "* -o location where the output file with inference needs to be stored. (results/core or results/xeon or results/gpu)\n",
    "* -d Type of Hardware Acceleration (CPU or GPU or MYRIAD)\n",
    "* -l Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so )\n",
    "\n",
    "\n",
    "### 1.1 Choosing Device\n",
    "\n",
    "First, we must select the device used for the inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "\n",
    "The plugin class is **`IEPlugin`** and can be constructed as follows:\n",
    "\n",
    "```python\n",
    "# Parsing arguments to the python script. Code for constructing argparser this is not shown here.\n",
    "args = argparser.parse_args()\n",
    "\n",
    "# Plugin initialization for specified device. 'plugin_dirs' is an optional argument.\n",
    "plugin = IEPlugin(device=args.device, plugin_dirs=args.plugin_dir)\n",
    "\n",
    "# Loading any additional exension libraries\n",
    "if args.cpu_extension and 'CPU' in args.device:\n",
    "    plugin.add_cpu_extension(args.cpu_extension)\n",
    "```\n",
    "**Note**\n",
    "Currently, three types of plugins are supported: CPU, GPU,  and MYRIAD\n",
    "CPU plugin may require additional extensions to improve performance.\n",
    "Use `add_cpu_extension` function to load these additional extensions.\n",
    "\n",
    "\n",
    "### 1.2 Read the IR (Intermediate Representation) model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different Intel hardware.\n",
    "We can import these optimized models (weights) into our neural network using **`IENetwork`**. \n",
    "```python\n",
    "# Importing network weights from IR models.\n",
    "net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Some layers in IR models may be unsupported by some plugins. \n",
    "if \"CPU\" in plugin.device:\n",
    "    supported_layers = plugin.get_supported_layers(net)\n",
    "    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(not_supported_layers) != 0:\n",
    "        log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                  format(plugin.device, ', '.join(not_supported_layers)))\n",
    "        log.error(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "        sys.exit(1)\n",
    "```\n",
    "\n",
    "### 1.3 Load the network on to the model\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**.\n",
    "\n",
    "```python\n",
    "# Loading IR model to the plugin.\n",
    "exec_net = plugin.load(network=net, num_requests=2)\n",
    "\n",
    "# Read and pre-process the input image/video.\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "n, c, h, w = net.inputs[input_blob]\n",
    "```\n",
    "\n",
    "```python\n",
    "# Determine the source of video, we will use pre-recorded input video file in this example, but it can be modified to use a camera with input argument 'cam'\n",
    "if args.input == 'cam':\n",
    "        input_stream = 0\n",
    "        out_file_name = 'cam'\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "```\n",
    "\n",
    "### 1.4 Start video capture using OpenCV \n",
    "\n",
    "Now we are ready to capture the frames from the video sample using **OpenCV VideoCapture** API.\n",
    "Upon getting the frame we are ready to perform inference.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the workload on the IoT DevCloud\n",
    "\n",
    "All the code up to this point is run within the notebook instance running on the Developer machine. Now we are ready run the inference which can be executed on an actual Edge Machine by creating a job to be submitted on a queue to run on the first available Edge hardware that matches the user choice.\n",
    "\n",
    "### 2.1 Creating job file\n",
    "The job file is written in bash, and will be executed directly on the compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"object_detection_job.sh\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $1\n",
    "\n",
    "# Running the object detection code\n",
    "python3 object_detection_demo_ssd_async.py -m /data/reference-sample-data/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i /data/reference-sample-data/object-detection-python/cars_1900.mp4 \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l /data/reference-sample-data/extension/libcpu_extension.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how job queue is submitted\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to compute nodes using the `qsub` command.\n",
    "We can submit object_detection_job to 4 different types of nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep properties | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the next 4 cells below will submit jobs to different nodes Core/Xeon/GPU or Myriad .\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 4 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter or control+enter** will run the cell and let you go to the next. So you can hit **shift+enter or control+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "#### submitting to a node with Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/core CPU FP32\" -N obj_det_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub object_detection_job.sh -l nodes=1:iei-tank-xeon -F \"results/xeon CPU FP32\" -N obj_det_xeon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU and using the onboard Intel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/gpu GPU FP32\" -N obj_det_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Movidius NCS (Neural Computing Stick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub object_detection_job.sh -l nodes=1:iei-tank-movidius -F \"results/myriad MYRIAD FP16\" -N obj_det_myriad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job: this is your current jupyter notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "**Note** currently it takes a anywhere from 3-5 minutes depending on the number of users accessing the edge nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View Results\n",
    "\n",
    "Once the jobs are completed, the stdout and stderr output stores in files with names of the form (based on our `-N` option):\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "But for this script, the main output is the mp4 videos which are stored in the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "See `demoutils.py` if interested in understanding further how the results are displayed in notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', 'results/core/cars_1900.mp4', 'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)','results/xeon/cars_1900.mp4','results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', 'results/gpu/cars_1900.mp4','results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Myriad (Intel Core + Movidius)','results/myriad/cars_1900.mp4','results/myriad/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
