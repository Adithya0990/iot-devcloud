{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo: Car Detection\n",
    "\n",
    "This is a sample reference implementation to showcase Object detection (car in this case) with SSD and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete but continue doing things ont he host while accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel infer requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of capturing.\n",
    "\n",
    "## Overview of How it works?\n",
    "At start-up the sample application reads the equivalent of command line arguments  and loads a network and image from the video input to the Inference Engine (IE) plugin. \n",
    "A job is submitted to the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Core CPU, Intel® Movidius™ and/or Neural Compute Stick)\n",
    "After the inference is completed, the output videos are appropriately stored in the /results directory which can then be viewed within the Jupyter Notebook instance\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed actual Edge hardware\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import some dependencies needed for displaying the results in this notebook\n",
    "(tip: use **crtl+enter** to run the cell but stay on the same cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demoutils import videoHTML,liveQstat, summaryPlot, progressIndicator\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If the user is curious to see the input video, run the following cell to view the orignal video used for inference and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('cars video', 'cars_1900.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using OpenVINO\n",
    "\n",
    "Let's first try running inference on a single image to see how OpenVINO works.\n",
    "We will be using OpenVINO's Inference Engine (IE) to locate vehicles on the road.\n",
    "There are five steps:\n",
    "\n",
    "1. Create a Intermediate Representation (IR) Model using the Intel Model Optimizer\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Creating IR Model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different Intel hardware.\n",
    "These models can be created from existsing DNN models from popular frameworks (e.g. Caffe, TF) using the Intel Model Optimizer. \n",
    "\n",
    "There is a utility script `model_downloader.py` to download some common modes. Run the following cel to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in commannd line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "We will be using the **mobilenet-ssd** model. This can be download with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will create it for you.\n",
    "\n",
    "There are more arguments to this script: you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "With the `-o` option set as above, this command download the model in the directory `raw_models`, with the `.caffemodel` located at `raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel`\n",
    "\n",
    "Let's now convert this to the optimized model using the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to '\\'.\n",
    "\n",
    "Here the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : outout dirctory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `models/mobilenet-ssd/FP32`\n",
    "\n",
    "There are two files:\n",
    "```\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "We will also be needing the FP16 version of the model for the MYRIAD runs. Run the dfollowing cell to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Choosing Device\n",
    "\n",
    "Now we must select the device used for the inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "\n",
    "The following cell constructs **`IEPlugin`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IEPlugin, IENetwork\n",
    "\n",
    "def createPlugin(device, extension_list):\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device='CPU')\n",
    "\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    for extension in extension_list:\n",
    "        plugin.add_cpu_extension('/data/reference-sample-data/extension/libcpu_extension.so')\n",
    "    \n",
    "    return plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Currently, three types of plugins are supported: CPU, GPU, and MYRIAD. CPU plugin may require additional extensions to improve performance, abd `add_cpu_extension` function is used to load these additional extensions.\n",
    "\n",
    "\n",
    "### 1.3 Read the IR (Intermediate Representation) model\n",
    "\n",
    "We can import optimized models (weights) from step 1.1 into our neural network using **`IENetwork`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "\n",
    "    net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "\n",
    "    # Some layers in IR models may be unsupported by some plugins. \n",
    "    if \"CPU\" in plugin.device:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            print(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "            return None\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Some models may be incompatible with some target devices. For example, some types of neural network layers are not supported on the CPU. \n",
    "\n",
    "### 1.4 Load the network on to the model\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetwork(plugin, net):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    return exec_net,input_blob,out_blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Run inference\n",
    "\n",
    "Now we are ready to try running the inference workload using the plugin.\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv2.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    return in_frame.reshape((n, c, h, w)),frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference, we will be running in **async_mode** by using `start_async` method. \n",
    "With the async_mode, the inference is started in parallel on either a separate thread or device.\n",
    "In other words, `start_async` is non-blocking and the main process is free to do any additional processing needed. \n",
    "In the next section, we will see an implementation of pipelining to mask the latency of loading and modifying images.\n",
    "\n",
    "During asynchronous runs, the different images are tracked by an integer `request_id`. \n",
    "Because we only have one image to process, we will just use 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labeling the image\n",
    "from out_process import placeBoxes\n",
    "\n",
    "\n",
    "# Request id to keep track of\n",
    "def runInference():\n",
    "    plugin = createPlugin(device='CPU', extension_list=['/data/reference-sample-data/extension/libcpu_extension.so'])\n",
    "    model_xml = \"models/mobilenet-ssd/FP32/mobilenet-ssd.xml\"\n",
    "    model_bin = \"models/mobilenet-ssd/FP32/mobilenet-ssd.bin\"\n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    exec_net,input_blob,out_blob = loadNetwork(plugin, net)\n",
    "    in_frame,original_frame = preprocessImage('cars_1900_first_frame.jpg', net, input_blob)\n",
    "    \n",
    "    my_request_id=0\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "\n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        prob_threshold = 0.5  # 50% confidence needed for \"detection\"\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "        frame = placeBoxes(res, None, prob_threshold, original_frame, initial_w, initial_h, False, my_request_id, 0)\n",
    "        # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "runInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference on a video\n",
    "\n",
    "Now that we know how to run inference on a single frame, let's extend this to multiple frames.\n",
    "This part is already implemented in [object_detection_demo_ssd_async.py](object_detection_demo_ssd_async.py).\n",
    "Most of the code is just an extension of the single frame example, but there are few sections of note that we highlight here.\n",
    "\n",
    "The following lines determine the source of video. We will use pre-recorded input video file in this example, but it can be modified to use a camera with input argument 'cam'.\n",
    "```python\n",
    "if args.input == 'cam':\n",
    "        input_stream = 0\n",
    "        out_file_name = 'cam'\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "```\n",
    "We capture frames from the video sample using **OpenCV VideoCapture** API.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```\n",
    "\n",
    "Finally, we have a latency masking scheme where we post-process the frames while another is being processed on the inference engine.\n",
    "\n",
    "```python\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "   # ... load next frame from cap ...\n",
    "\n",
    "   # start the next frame\n",
    "   exec_net.start_async(request_id=next_request_id, inputs={input_blob: in_frame})\n",
    "\n",
    "   # see if the current frame is ready\n",
    "   if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "       # ... post processing current frame ...\n",
    "    \n",
    "   # swap request ids\n",
    "   cur_request_id, next_request_id = next_request_id, cur_request_id\n",
    "```\n",
    "\n",
    "The python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=\"/data/reference-sample-data\"\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i ${SAMPLEPATH}/object-detection-python/cars_1900.mp4 \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l ${SAMPLEPATH}/extension/libcpu_extension.so\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **mobilenet-ssd** pre-trained model which has been pre-processed using the **model optimzer**\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "   (**Note** we are using mobilenet-ssd in this example. However, OpenVINO's Inference Engine is compatible with other neural network architectures such as AlexNet*, GoogleNet*, MxNet* etc.,)    \n",
    "\n",
    "* -i location of the input video stream (video/cars_1900.mp4)\n",
    "* -o location where the output file with inference needs to be stored. (results/core or results/xeon or results/gpu)\n",
    "* -d Type of Hardware Acceleration (CPU or GPU or MYRIAD)\n",
    "* -l Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so )\n",
    "\n",
    "### 2.1 Creating job file\n",
    "\n",
    "All the code up to this point is run within the notebook instance running on the Developer machine. \n",
    "But to run inference on the entire video, we need more compute power.\n",
    "We will run the workload on an actual Edge Machine by creating a job to be submitted on a queue to run on the first available Edge hardware that matches the user choice.\n",
    "\n",
    "The job file is written in bash, and will be executed directly on the compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"object_detection_job.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the output video specs:\n",
    "Set the output video specifications in terms of video resolution and number of frames post processed.\n",
    "\n",
    "- Output video resolution and number of frames are same as the input video (Slowest option)\n",
    "`set RESOLUTION=1.0 and SKIPFRAME=1`.\n",
    "\n",
    "- Output video resolution is less than the input video by x where 1>=x>=0 and number of frames is unchanged\n",
    "`set RESOLUTION=x and SKIPFRAME=1`.\n",
    "\n",
    "- Output video resolution is less than the input video by x where 1>=x>=0 and number of frames is half the number of frames in the input video (fastest option)\n",
    "`set RESOLUTION=x and SKIPFRAME=2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $1\n",
    "\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    source /opt/intel/computer_vision_sdk/bin/setup_hddl.sh\n",
    "    aocl program acl0 /opt/intel/computer_vision_sdk_2018.4.420/bitstreams/a10_vision_design_bitstreams/4-0_PL1_FP11_MobileNet_ResNet_VGG_Clamp.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i /data/reference-sample-data/object-detection-python/cars_1900.mp4 \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l /data/reference-sample-data/extension/libcpu_extension.so\n",
    "\n",
    "g++ -std=c++14 out_process.cpp -o out_process  -lopencv_core -lopencv_videoio -lopencv_imgproc -lopencv_highgui  -fopenmp -I/opt/intel/computer_vision_sdk/opencv/include/ -L/opt/intel/computer_vision_sdk/opencv/lib/\n",
    "\n",
    "#$skip_frame (1->no-skip, 2->skip 1 frame)\n",
    "#$resolution (float) 1.0, 0.75, 0.5\n",
    "#resolution 0.75: the output resolution equals to 75% of the input resolution\n",
    "#resolution 1.0: the output resolution equals to the input video resolution\n",
    "#resolution 0.5: the output resolution equals to 50% of the input resolution\n",
    "SKIPFRAME=1\n",
    "RESOLUTION=0.5\n",
    "./out_process \"/data/reference-sample-data/object-detection-python/cars_1900.mp4\" $1 $SKIPFRAME $RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how job queue is submitted\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to compute nodes using the `qsub` command.\n",
    "We can submit object_detection_job to 4 different types of nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep properties | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the next 4 cells below will submit jobs to different nodes Core/Xeon/GPU/FPGA or Myriad .\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 4 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/core CPU FP32\" -N obj_det_core\n",
    "    \n",
    "#Progress files\n",
    "if job_id_core:\n",
    "    infer_progress = os.path.join('results/core', 'i_progress_'+job_id_core[0]+'.txt')\n",
    "    render_progress = os.path.join('results/core', 'v_progress_'+job_id_core[0]+'.txt')\n",
    "    infer = open(infer_progress, \"w\")    \n",
    "    render = open(render_progress, \"w\")\n",
    "    infer.close()\n",
    "    render.close()\n",
    "    \n",
    "    #Progress indicators\n",
    "    progressIndicator(infer_progress,\"Inference\", 0, 100)\n",
    "    progressIndicator(render_progress,\"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub object_detection_job.sh -l nodes=1:iei-tank-xeon -F \"results/xeon CPU FP32\" -N obj_det_xeon \n",
    "\n",
    "#Progress files\n",
    "if job_id_xeon:\n",
    "    infer_progress = os.path.join('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt')\n",
    "    render_progress = os.path.join('results/xeon', 'v_progress_'+job_id_xeon[0]+'.txt')\n",
    "    infer = open(infer_progress, \"w\")    \n",
    "    render = open(render_progress, \"w\")\n",
    "    infer.close()\n",
    "    render.close()\n",
    "    \n",
    "    #Progress indicators\n",
    "    progressIndicator(infer_progress,\"Inference\", 0, 100)\n",
    "    progressIndicator(render_progress,\"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU and using the onboard Intel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/gpu GPU FP32\" -N obj_det_gpu \n",
    "\n",
    "#Progress files\n",
    "if job_id_gpu:\n",
    "    infer_progress = os.path.join('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt')\n",
    "    render_progress = os.path.join('results/gpu', 'v_progress_'+job_id_gpu[0]+'.txt')\n",
    "    infer = open(infer_progress, \"w\")    \n",
    "    render = open(render_progress, \"w\")\n",
    "    infer.close()\n",
    "    render.close()\n",
    "    \n",
    "    #Progress indicators\n",
    "    progressIndicator(infer_progress,\"Inference\", 0, 100)\n",
    "    progressIndicator(render_progress,\"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Movidius NCS (Neural Computing Stick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_myriad = !qsub object_detection_job.sh -l nodes=1:iei-tank-movidius -F \"results/myriad MYRIAD FP16\" -N obj_det_myriad\n",
    "\n",
    "#Progress files\n",
    "if job_id_myriad:\n",
    "    infer_progress = os.path.join('results/myriad', 'i_progress_'+job_id_myriad[0]+'.txt')\n",
    "    render_progress = os.path.join('results/myriad', 'v_progress_'+job_id_myriad[0]+'.txt')\n",
    "    infer = open(infer_progress, \"w\")    \n",
    "    render = open(render_progress, \"w\")\n",
    "    infer.close()\n",
    "    render.close()\n",
    "    \n",
    "    #Progress indicators\n",
    "    progressIndicator(infer_progress,\"Inference\", 0, 100)\n",
    "    progressIndicator(render_progress,\"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel FPGA HDDL-F (High Density Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_fpga = !qsub object_detection_job.sh -l nodes=1:iei-tank-fpga -F \"results/fpga HETERO:FPGA,CPU FP32\" -N obj_det_fpga\n",
    "\n",
    "#Progress files\n",
    "if job_id_fpga:\n",
    "    infer_progress = os.path.join('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt')\n",
    "    render_progress = os.path.join('results/fpga', 'v_progress_'+job_id_fpga[0]+'.txt')\n",
    "    infer = open(infer_progress, \"w\")    \n",
    "    render = open(render_progress, \"w\")\n",
    "    infer.close()\n",
    "    render.close()\n",
    "    \n",
    "    #Progress indicators\n",
    "    progressIndicator(infer_progress,\"Inference\", 0, 100)\n",
    "    progressIndicator(render_progress,\"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job: this is your current jupyter notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "**Note** currently it takes a anywhere from 3-5 minutes depending on the number of users accessing the edge nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the stdout and stderr output stores in files with names of the form (based on our `-N` option):\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "But for this script, the main output is the mp4 videos which are stored in the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "See `demoutils.py` if interested in understanding further how the results are displayed in notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', 'results/core/cars_1900_output_'+job_id_core[0]+'.mp4', 'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)','results/xeon/cars_1900_output_'+job_id_xeon[0]+'.mp4','results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', 'results/gpu/cars_1900_output_'+job_id_gpu[0]+'.mp4','results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel FPGA HDDL-F','results/fpga/cars_1900_output_'+job_id_fpga[0]+'.mp4','results/fpga/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + (Intel CPU + Movidius)','results/myriad/cars_1900_output_'+job_id_myriad[0]+'.mp4','results/myriad/stats.txt'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPlot({'results/core/stats.txt':'Core', 'results/xeon/stats.txt':'Xeon', 'results/gpu/stats.txt':'GPU', 'results/fpga/stats.txt':'FPGA', 'results/myriad/stats.txt':'Myriad'}, 'Architecture', 'Time, seconds', 'Inference engine processing time' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
