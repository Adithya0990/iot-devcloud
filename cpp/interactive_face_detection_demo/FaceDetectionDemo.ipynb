{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Face Detection Sample\n",
    "\n",
    "This sample showcases Object Detection task applied for face recognition using sequence of neural networks.\n",
    "Async API usage can improve overall frame-rate of the application, because rather than wait for inference to complete,\n",
    "the application can continue operating on the host while accelerator is busy.\n",
    "This sample maintains three parallel infer requests for the Age/Gender Recognition, Head Pose Estimation, and Emotions Recognition that run simultaneously.\n",
    "\n",
    "Other sample objectives are:\n",
    "\n",
    "*\tVideo as input support via OpenCV\n",
    "*\tVisualization of the resulting face bounding boxes from Face Detection network\n",
    "*\tVisualization of age/gender, head pose and emotion information for each detected face\n",
    "\n",
    "OpenCV\\* is used to draw resulting bounding boxes, labels, and other information. You can copy and paste this code without pulling Inference Engine sample helpers into your application\n",
    "\n",
    "## How it Works\n",
    "\n",
    "*\tThe application reads command line parameters and loads up to four networks depending on `-m...` options family to the Inference Engine.\n",
    "*\tThe application gets a frame from the OpenCV's VideoCapture.\n",
    "*\tThe application performs inference on the frame detection network.\n",
    "*\tThe application performs three simultaneous inferences, using the Age/Gender, Head Pose and Emotions detection networks if they are specified in command line.\n",
    "*\tThe application displays the results.\n",
    "\n",
    "The new Async API operates with a new notion of the Infer Request that encapsulates the inputs/outputs and separates scheduling and waiting for result. For more information about Async API and the difference between Sync and Async modes performance, refer to **How it Works** and **Async API** sections in [Object Detection SSD, Async API Performance Showcase Sample](@ref InferenceEngineObjectDetectionSSDDemoAsyncApplication).\n",
    "\n",
    "\n",
    "## Compiling\n",
    "\n",
    "There are two source files: \n",
    "* [detectors.cpp](detectors.cpp) -- contains the various helper functions\n",
    "* [main.cpp](main.cpp) -- contains the main loop\n",
    "\n",
    "We have provided a Makefile for compiling the application. Run the following cell to run the make command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces an executable [face_detector](). This executable takes in a number of different command line arguments.\n",
    "\n",
    "Run the following cell to see the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/data/reference-sample-data/extension/ \n",
    "./face_detector -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of the cpp file here is a slightly modified version of the faca_detector code built-in to the OpnVINO distribution.\n",
    "In this version, the result is written into a output mp4 file specified by the `-o` flag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the inference\n",
    "\n",
    "Now we are ready to run the inference workload. In this step we will be submitting the workload as a job to the job queue.\n",
    "\n",
    "Currently, you are on what is called a \"devnode\". On this system, you are alloated just one core on a large Xeon CPU. The purpose of this node is to develop code and run minimal jupyter notebooks, but it is not meant for compute jobs like deep learning inference. So we need to request additional resources from the cluster to run the inference, and this is done through the job queue.\n",
    "\n",
    "To put an item on the job queue, we must first create a bash script that run the workload we want. Run the following cell to create bash script [face_detection_demo.sh](face_detection_demo.sh) which will be our job script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile face_detection_demo.sh\n",
    "#PBS\n",
    "if [ \"$2\" == \"MYRIAD\" ]; then\n",
    "  FP=\"FP16\"\n",
    "else\n",
    "  FP=\"FP32\"\n",
    "fi\n",
    "\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    source /opt/intel/computer_vision_sdk/bin/setup_hddl.sh\n",
    "    aocl program acl0 /opt/intel/computer_vision_sdk_2018.4.420/bitstreams/a10_vision_design_bitstreams/4-0_PL1_FP11_MobileNet_ResNet_VGG_Clamp.aocx\n",
    "fi\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/data/reference-sample-data/extension/ \n",
    "MODEL_ROOT=/opt/intel/computer_vision_sdk/deployment_tools/intel_models/\n",
    "./face_detector -i $1 -no_wait \\\n",
    "-m ${MODEL_ROOT}/face-detection-adas-0001/$FP/face-detection-adas-0001.xml \\\n",
    "-m_hp ${MODEL_ROOT}/head-pose-estimation-adas-0001/$FP/head-pose-estimation-adas-0001.xml \\\n",
    "-m_ag ${MODEL_ROOT}/age-gender-recognition-retail-0013/$FP/age-gender-recognition-retail-0013.xml \\\n",
    "-m_em ${MODEL_ROOT}/emotions-recognition-retail-0003/$FP/emotions-recognition-retail-0003.xml \\\n",
    "-d $2 \\\n",
    "-o $3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put this script on the job queue, we use the command `qsub`.\n",
    "There are two important arguments we use with this command.\n",
    "\n",
    "First, the `-l` flag.\n",
    "This flag is used to specify what type of resources to request from the cluster.\n",
    "For example this can be used to request a Intel Xeon system, or it can be used to request a system with an FPGA.\n",
    "The syntax is `-l nodes=1:<tag>` where `<tag>` is the descriptor tag for the resource you want.\n",
    "For example, `-l nodes=1:iei-tank-xeon` will request an Intel Xeon system.\n",
    "To see the list of available tags, and the number of avilable systems, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep properties | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there is the `-F` flag, which is used to pass in arguments to the job script.\n",
    "The [face_detection_demo.sh](face_detection_demo.sh) takes in 2 arguments:\n",
    "1) the path to the video to run inference on\n",
    "2) targeted device (CPU,GPU,MYRIAD)\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "The following line will request an Intel Xeon system, and passes in \"faces-recognition-walking.mp4 CPU\" to the job script. Run the cell to submit this job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub face_detection_demo.sh -l nodes=1:iei-tank-xeon -F \"faces-recognition-walking-and-pause.mp4 CPU results/xeon\"\n",
    "print(job_id_xeon[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should see a output like \"{job_id}.c003\", where {job_id} is a number.\n",
    "This is your job ID, and this value can be used to check on the progress of the job down the line.\n",
    "Furthermore, the above job script has been written so that it uses its job_id as the name of the output video.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One bigadvantage of the Job queue system is that you may submit multiple jobs at once. \n",
    "These jobs will be run as soon as resources are available, and may all run at once if the cluster is not busy.\n",
    "Here are few other \"preset\" jobs that for your convenience that you can run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub face_detection_demo.sh -l nodes=1:iei-tank-core -F \"faces-recognition-walking.mp4 CPU results/core\"\n",
    "print(job_id_core[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU and using the onboard Intel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Core CPU with Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub face_detection_demo.sh -l nodes=1:iei-tank-core -F \"head-pose-female-male.mp4 GPU results/gpu\"\n",
    "print(job_id_gpu[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Movidius NCS (Neural Computing Stick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to node with Intel Movidius NCS...\")\n",
    "#Submit job to the queue\n",
    "job_id_myriad = !qsub face_detection_demo.sh -l nodes=1:iei-tank-movidius -F \"faces-recognition-walking.mp4 MYRIAD results/myriad\"\n",
    "print(job_id_myriad[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_myriad:\n",
    "    progressIndicator('results/myriad', 'i_progress_'+job_id_myriad[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel FPGA HDDL-F (High Density Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to node with Intel FPGA HDDL-F...\")\n",
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub face_detection_demo.sh -l nodes=1:iei-tank-fpga -F \"head-pose-female-male.mp4 HETERO:FPGA,CPU results/fpga\"\n",
    "print(job_id_fpga[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `qstat` command is used to track the progress of the jobs. \n",
    "We have provided a utility funtion \"liveQstat()\" that provide a live updating GUI for you.\n",
    "Run the following cell to check on the progress of your earlier jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jobs are done. The following cell can be used to display the output. \n",
    "The videoHTML is a utility fnction provided in [demoutils.py](demoutils.py).\n",
    "This takes one argument, which is the path to the video (see above for convention for the path name).\n",
    "\n",
    "For your convenience we have stored the jobid from the above `qsub` commands, and the following cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)',['results/xeon/output_'+job_id_xeon[0]+'.mp4'], 'results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', ['results/core/output_'+job_id_core[0]+'.mp4'], 'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', ['results/gpu/output_'+job_id_gpu[0]+'.mp4'], 'results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel FPGA HDDL-F',['results/fpga/output_'+job_id_fpga[0]+'.mp4'], 'results/fpga/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + (Intel CPU + Movidius)',['results/myriad/output_'+job_id_myriad[0]+'.mp4'], 'results/myriad/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPlot({'results/core/stats.txt':'Core', 'results/xeon/stats.txt':'Xeon', 'results/gpu/stats.txt':'GPU', 'results/fpga/stats.txt':'FPGA', 'results/myriad/stats.txt':'Myriad'}, 'Architecture', 'Time, seconds', 'Inference engine processing time' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
